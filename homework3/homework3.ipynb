{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Продвинутое машинное обучение: \n",
    "# Домашнее задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Третье домашнее задание посвящено достаточно простой, но, надеюсь, интересной задаче, в которой потребуется творчески применить методы сэмплирования. Как и раньше, в качестве решения ожидается ссылка на jupyter-ноутбук на вашем github (или публичный, или с доступом для snikolenko); ссылку обязательно нужно прислать в виде сданного домашнего задания на портале Академии. Как всегда, любые комментарии, новые идеи и рассуждения на тему категорически приветствуются. \n",
    "В этом небольшом домашнем задании мы попробуем улучшить метод Шерлока Холмса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пользовался он для этого так называемым частотным методом: смотрел, какие буквы чаще встречаются в зашифрованных текстах, и пытался подставить буквы в соответствии с частотной таблицей: E — самая частая и так далее.\n",
    "В этом задании мы будем разрабатывать более современный и продвинутый вариант такого частотного метода. В качестве корпусов текстов для подсчётов частот можете взять что угодно, но для удобства вот вам “Война и мир” по-русски и по-английски:\n",
    "https://www.dropbox.com/s/k23enjvr3fb40o5/corpora.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Реализуйте базовый частотный метод по Шерлоку Холмсу:\n",
    "- подсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "- возьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе вряд ли сработает), зашифруйте их посредством случайной перестановки символов;\n",
    "- расшифруйте их таким частотным методом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import random\n",
    "import collections\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from Levenshtein import ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение zip архива\n",
    "corpora_zip = zipfile.ZipFile('corpora.zip')\n",
    "\n",
    "# загрузка файла AnnaKarenina\n",
    "with corpora_zip.open('AnnaKarenina.txt', 'r') as readFile:\n",
    "    karenina_text = readFile.read().decode('utf8')\n",
    "# загрузка файла AnnaKarenina\n",
    "with corpora_zip.open('WarAndPeace.txt', 'r') as readFile:\n",
    "    war_peace_text = readFile.read().decode('utf8')\n",
    "# загрузка файла AnnaKarenina\n",
    "with corpora_zip.open('WarAndPeaceEng.txt', 'r') as readFile:\n",
    "    war_peace_eng_text = readFile.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, language):\n",
    "    \"\"\"Обработка текста.\"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    if language == 'russian':\n",
    "        text = re.sub(r'[^а-яА-ЯыъьёЁ]', \" \", text)\n",
    "    else:\n",
    "        text = re.sub(r'[^a-zA-Z]', \" \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обрабатываем все файлы\n",
    "karenina_text = preprocessing(karenina_text, language='russian')\n",
    "war_peace_text = preprocessing(war_peace_text, language='russian')\n",
    "war_peace_eng_text = preprocessing(war_peace_eng_text, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подсчитаем частоты букв по корпусам\n",
    "val_freq = collections.Counter(karenina_text)\n",
    "karenina_freq = {key: val for key, val in val_freq.items() if key in set(karenina_text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальный текст: левин думал о евангельском изречении не потому что\n",
      "Закодированный текст: фрлпнгтекжфгэгрлжньрфчвхэкгпюмрырнппгнргдэзэкегызэ\n"
     ]
    }
   ],
   "source": [
    "# берем рандомный текст и делаем перестановку символов\n",
    "random_text_1 = \"\"\"Левин думал о евангельском изречении не потому, чтоб он считал себя премудрым. Он не считал себя премудрым, но не мог не знать, что он был умнее жены и Агафьи Михайловны, и не мог не знать того, что, когда он думал о смерти, он думал всеми силами души. Он знал тоже, что многие мужские большие умы, мысли которых об этом он читал, думали об этом и не знали одной сотой того, что знала об этом его жена и Агафья Михайловна. Как ни различны были эти две женщины, Агафья Михайловна и Катя, как ее называл брат Николай и как теперь Левину было особенно приятно называть ее, они в этом были совершенно похожи. Обе несомненно знали, что такое была жизнь и что такое была смерть, и хотя никак не могли ответить и не поняли бы даже тех вопросов, которые представлялись Левину, обе не сомневались в значении этого явления и совершенно одинаково, не только между собой, но разделяя этот взгляд с миллионами людей, смотрели на это. Доказательство того, что они знали твердо, что такое была смерть, состояло в том, что они, ни секунды не сомневаясь, знали, как надо действовать с умирающими, и не боялись их. Левин же и другие, хотя и многое могли сказать о смерти, очевидно не знали, потому что боялись смерти и решительно не знали, что надо делать, когда люди умирают. Если бы Левин был теперь один с братом Николаем, он бы с ужасом смотрел на него и еще с бóльшим ужасом ждал, и больше ничего бы не умел сделать.\n",
    "Мало того, он не знал, что говорить, как смотреть, как ходить. Говорить о постороннем ему казалось оскорбительным, нельзя; говорить о смерти, о мрачном – тоже нельзя. Молчать – тоже нельзя. «Смотреть – он подумает, что я изучаю его, боюсь; не смотреть – он подумает, что я о другом думаю. Ходить на цыпочках – он будет недоволен; на всю ногу – совестно». Кити же, очевидно, не думала и не имела времени думать о себе; она думала о нем, потому что знала что-то, и все выходило хорошо. Она и про себя рассказывала и про свою свадьбу, и улыбалась, и жалела, и ласкала его, и говорила о случаях выздоровления, и все выходило хорошо; стало быть, она знала. Доказательством того, что деятельность ее и Агафьи Михайловны была не инстинктивная, животная, неразумная, было то, что, кроме физического ухода, облегчения страданий, и Агафья Михайловна и Кити требовали для умирающего еще чего-то такого, более важного, чем физический уход, и чего-то такого, что не имело ничего общего с условиями физическими. Агафья Михайловна, говоря об умершем старике, сказала: «Что ж, слава Богу, причастили, соборовали, дай Бог каждому так умереть». Катя точно так же, кроме всех забот о белье, пролежнях, питье, в первый же день успела уговорить больного в необходимости причаститься и собороваться.\"\"\"\n",
    "random_text_2 = \"\"\"Вернувшись от больного на ночь в свои два нумера, Левин сидел, опустив голову, не зная, что делать. Не говоря уже о том, чтоб ужинать, устраиваться на ночлег, обдумывать, что они будут делать, он даже и говорить с женою не мог: ему совестно было. Кити же, напротив, была деятельнее обыкновенного. Она даже была оживленнее обыкновенного. Она велела принести ужинать, сама разобрала вещи, сама помогла стлать постели и не забыла обсыпать их персидским порошком. В ней было возбуждение и быстрота соображения, которые появляются у мужчин пред сражением, борьбой, в опасные и решительные минуты жизни, те минуты, когда раз навсегда мужчина показывает свою цену и то, что все прошедшее его было не даром, а приготовлением к этим минутам.\n",
    "Все дело спорилось у нее, и еще не было двенадцати, как все вещи были разобраны чисто, аккуратно, как-то так особенно, что нумер стал похож на дом, на ее комнаты: постели постланы, щетки, гребни, зеркальца выложены, салфеточки постланы.\n",
    "Левин находил, что непростительно есть, спать, говорить даже теперь, и чувствовал, что каждое движение его было неприлично. Она же разбирала щеточки, но делала все это так, что ничего в этом оскорбительного не было.\n",
    "Есть, однако, они ничего не могли, и долго не могли заснуть, и даже долго не ложились спать.\"\"\"\n",
    "\n",
    "# предобработаем их\n",
    "random_text_1 = preprocessing(random_text_1, language='russian')\n",
    "random_text_2 = preprocessing(random_text_2, language='russian')\n",
    "\n",
    "# перемешиваем и получаем рандомные значений\n",
    "shuffle_text = sorted(list(set(karenina_text)))\n",
    "# перемешиваем\n",
    "random.shuffle(shuffle_text)\n",
    "encoding_text = {key: value for key, value in zip(sorted(list(set(karenina_text))), shuffle_text)}\n",
    "\n",
    "# кодируем текст\n",
    "encoding_text_1 = ''.join(encoding_text[txt] for txt in random_text_1)\n",
    "encoding_text_2 = ''.join(encoding_text[txt] for txt in random_text_2)\n",
    "print(f\"Оригинальный текст: {random_text_1[:50]}\\nЗакодированный текст: {encoding_text_1[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# декодируем последовательность\n",
    "def decode_text(text, dict_text):\n",
    "    \"\"\"Декодирование последовательности.\"\"\"\n",
    "    \n",
    "    val_freq = collections.Counter(text)\n",
    "    text_freq = {key: val for key, val in val_freq.items() if key in set(karenina_text)}\n",
    "    sort_text_freq = dict(val_freq.most_common())\n",
    "    letter_pairs = {k: v for k, v in zip(sort_text_freq, dict_text)}\n",
    "    decoded_text = ''.join(letter_pairs[let] for let in text)\n",
    "    \n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст:серни мьлас о ераияесдвуол нбкеыеинн ие жотоль ыто\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.5097503900156006\n",
      "\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст:вераявйилм от досмаого ан аочм в лвои квн аяперн с\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 0.5967346938775511\n"
     ]
    }
   ],
   "source": [
    "# формируем словарь \n",
    "dict_text = dict(val_freq.most_common())\n",
    "decoding_text_1 = decode_text(encoding_text_1, dict_text)\n",
    "decoding_text_2 = decode_text(encoding_text_2, dict_text)\n",
    "print(f\"Пример 1:\\nОригинальный текст:{random_text_1[:50]}\\nДекодированный текст:{decoding_text_1[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 1 примера: {ratio(random_text_1, decoding_text_1)}\")\n",
    "print(f\"\\nПример 2:\\nОригинальный текст:{random_text_2[:50]}\\nДекодированный текст:{decoding_text_2[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 2 примера: {ratio(random_text_2, decoding_text_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод: фразы распознаются, но довольно трудно понять смысл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:\n",
    "- подсчитайте частоты биграмм (т.е. пар последовательных букв) по корпусам;\n",
    "- проведите тестирование аналогично п.1, но при помощи биграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем словарь для биграмм\n",
    "def to_bigramm(text):\n",
    "    \"\"\"Формирование биграмм.\"\"\"\n",
    "    \n",
    "    bigram_freq = {}\n",
    "    len_text = len(text)\n",
    "    for iterr in range(len_text - 1):\n",
    "        # формируем биграмы по 2 символам\n",
    "        bigram = text[iterr:iterr + 2]\n",
    "        # смотрим их наличие в словаре\n",
    "        if bigram_freq.get(bigram, None):\n",
    "            bigram_freq[bigram] += 1\n",
    "        else:\n",
    "            bigram_freq[bigram] = 1\n",
    "    \n",
    "    # формируем словарь с биграмми в виде ключа и значениями\n",
    "    bigram_freq = dict(sorted(bigram_freq.items(), key=lambda val: val[1], reverse=True))\n",
    "    \n",
    "    return bigram_freq\n",
    "\n",
    "# биграмы для всего текста\n",
    "bigram_freq = to_bigramm(karenina_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_bigram(text, dict_text):\n",
    "    \"\"\"Функция для декодирования тестовых текстов.\"\"\"\n",
    "\n",
    "    # биграмы для входного текста\n",
    "    text_freq = to_bigramm(text)\n",
    "    decoder_text = {key: value for key, value in zip(text_freq, dict_text)}\n",
    "    len_text = len(text)\n",
    "    text_new = []\n",
    "    for iterr in range(0, len_text - 1, 2):\n",
    "        text_new.append(text[iterr:iterr + 2])\n",
    "    # формируем декодированный текст\n",
    "    text = ''.join(decoder_text[bigram] for bigram in text_new)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по биграммам:деасраетчтеро быопач мженитоехбин ув на ити  г ки \n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.4079563182527301\n",
      "\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по биграммам:у внруше э вс нытенаста  ннаош ть  ми нн н ч яниэт\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 0.4115965700285831\n"
     ]
    }
   ],
   "source": [
    "# декодируем текст\n",
    "decoding_bigram_1 = decode_bigram(encoding_text_1, bigram_freq)\n",
    "decoding_bigram_2 = decode_bigram(encoding_text_2, bigram_freq)\n",
    "print(f\"Пример 1:\\nОригинальный текст:{random_text_1[:50]}\\nДекодированный текст по биграммам:{decoding_bigram_1[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 1 примера: {ratio(random_text_1, decoding_bigram_1)}\")\n",
    "print(f\"\\nПример 2:\\nОригинальный текст:{random_text_2[:50]}\\nДекодированный текст по биграммам:{decoding_bigram_2[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 2 примера: {ratio(random_text_2, decoding_bigram_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод: результаты стали хуже, кодировалось по одному методу, а декодируется по другому"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "- предложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "- реализуйте и протестируйте его, убедитесь, что результаты улучшились."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC-сэмплирование\n",
    "# все символы текста\n",
    "alphabet = ''.join(sorted(list(set(karenina_text))))\n",
    "val_alphabet = len(alphabet)\n",
    "# преобразуем символы в индекс\n",
    "char_to_index = {val: key for key, val in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mcc(text, char_to_index, alphabet, val_alphabet):\n",
    "    \"\"\"Обучение mcmc.\"\"\"\n",
    "    \n",
    "    # формируем матрицу\n",
    "    matrix_val = np.zeros((val_alphabet, val_alphabet))\n",
    "    len_text = len(text)\n",
    "    for iterr in range(len_text - 1):\n",
    "        matrix_val[char_to_index[text[iterr]], char_to_index[text[iterr + 1]]] += 1\n",
    "    matrix_val = np.clip(matrix_val, 1, None)\n",
    "    matrix_val = (np.log(matrix_val).T - np.log(matrix_val.sum(axis=1))).T\n",
    "    \n",
    "    return matrix_val\n",
    "\n",
    "# определяем матрицу признаков\n",
    "matrix_val = train_mcc(karenina_text, char_to_index, alphabet, val_alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(text, alphabet, matrix_val, char_to_index, transpose):\n",
    "    \"\"\"Определяем функцию правдоподобия.\"\"\"\n",
    "\n",
    "    # определяем текст\n",
    "    translation = str.maketrans(alphabet, ''.join(transpose))\n",
    "    text = text.translate(translation)\n",
    "\n",
    "    # считаем логарифм правдоподобия\n",
    "    likelihood = 0\n",
    "    for iterr in range(len(text) - 1):\n",
    "        likelihood += matrix_val[char_to_index[text[iterr]], char_to_index[text[iterr + 1]]]\n",
    "\n",
    "    return likelihood\n",
    "    \n",
    "def decode_mcmc(text, alphabet, matrix_val, char_to_index, iteration):\n",
    "    \"\"\"Функция декодирования по MCMC.\"\"\"\n",
    "    \n",
    "    transpose = np.array(list(alphabet))\n",
    "    random.shuffle(transpose)\n",
    "    value_likelihood = log_likelihood(text, alphabet, matrix_val, char_to_index, transpose)\n",
    "    best_value = value_likelihood\n",
    "    best_transpose = transpose.copy()\n",
    "    for i in range(iteration):\n",
    "        pass_index = random.sample(range(len(alphabet)), 2)\n",
    "        transpose[pass_index[0]], transpose[pass_index[1]] = transpose[pass_index[1]], transpose[pass_index[0]]\n",
    "        new_likelihood = log_likelihood(text, alphabet, matrix_val, char_to_index, transpose)\n",
    "        if new_likelihood >= value_likelihood:\n",
    "            value_likelihood = new_likelihood\n",
    "            if new_likelihood > best_value:\n",
    "                best_value = new_likelihood\n",
    "                best_transpose = transpose.copy()\n",
    "        else:\n",
    "            if random.random() < np.exp(new_likelihood - value_likelihood):\n",
    "                value_likelihood = new_likelihood\n",
    "            else:\n",
    "                transpose[pass_index[0]], transpose[pass_index[1]] = transpose[pass_index[1]], transpose[pass_index[0]]\n",
    "\n",
    "    # определяем текст\n",
    "    translation_out = str.maketrans(alphabet, ''.join(best_transpose))\n",
    "    answer = text.translate(translation_out)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по mcmc:левин думал о евангельском изречении не потому что\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 1.0\n"
     ]
    }
   ],
   "source": [
    "# декодируем последовательность\n",
    "decoded_mcc_text_1 = decode_mcmc(encoding_text_1, alphabet, matrix_val, char_to_index, iteration=200000)\n",
    "print(f\"Пример 1:\\nОригинальный текст:{random_text_1[:50]}\\nДекодированный текст по mcmc:{decoded_mcc_text_1[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 1 примера: {ratio(random_text_1, decoded_mcc_text_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по mcmc:вернувшись от больного на ночь в свои два нумера л\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 1.0\n"
     ]
    }
   ],
   "source": [
    "# декодируем последовательность\n",
    "decoded_mcc_text_2 = decode_mcmc(encoding_text_2, alphabet, matrix_val, char_to_index, iteration=200000)\n",
    "print(f\"Пример 2:\\nОригинальный текст:{random_text_2[:50]}\\nДекодированный текст по mcmc:{decoded_mcc_text_2[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 1 примера: {ratio(random_text_2, decoded_mcc_text_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Расшифруйте сообщение:\n",
    "←⇠⇒↟↹↷⇊↹↷↟↤↟↨←↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↟⇒↟↹⇷⇛⇞↨↟↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↨←⇌⇠↨↹⇙↹⇸↨⇛↙⇛↹⇠⇛⇛↲⇆←↝↟↞↹⇌⇛↨⇛⇯⇊↾↹⇒←↙⇌⇛↹⇷⇯⇛⇞↟↨⇴↨⇈↹⇠⇌⇛⇯←←↹↷⇠←↙⇛↹↷⇊↹↷⇠←↹⇠↤←⇒⇴⇒↟↹⇷⇯⇴↷↟⇒⇈↝⇛↹↟↹⇷⇛⇒⇙⇞↟↨←↹↳⇴⇌⇠↟↳⇴⇒⇈↝⇊↾↹↲⇴⇒⇒↹⇰⇴↹⇷⇛⇠⇒←↤↝←←↹⇞←↨↷←⇯↨⇛←↹⇰⇴↤⇴↝↟←↹⇌⇙⇯⇠⇴↹↘⇛↨↞↹⇌⇛↝←⇞↝⇛↹↞↹↝↟⇞←↙⇛↹↝←↹⇛↲←⇆⇴⇏\n",
    "Или это (они одинаковые, второй вариант просто на случай проблем с юникодом):\n",
    "დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭჾႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭჾႣჵისႼჰႨჲდႩჳჲႨჇႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵჇႧჂჲდႨჾႣႩჳჂჾႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხსდდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩჇႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 2:\n",
      "Оригинальный текст:დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭჾႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭჾႣჵისႼჰႨჲდႩჳჲႨჇႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵჇႧჂჲდႨჾႣႩჳჂჾႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხსდდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩჇႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ\n",
      "Декодированный текст по mcmc:если вы вимите нордальный или почти нордальный текст у этого сообщения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный балл за послемнее четвертое замание курса хотя конечно я ничего не обещаж\n"
     ]
    }
   ],
   "source": [
    "# текстовое сообщение\n",
    "text_message = \"დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭჾႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭჾႣჵისႼჰႨჲდႩჳჲႨჇႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵჇႧჂჲდႨჾႣႩჳჂჾႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხსდდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩჇႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ\"\n",
    "# преобразуем последовательность\n",
    "list_message_symbol = list(set(text_message))\n",
    "decoder_alphabet = {key: value for key, value in zip(list_message_symbol, alphabet)}\n",
    "encode_message = ''.join(decoder_alphabet[char] for char in text_message)\n",
    "# декодируем сообщение\n",
    "message_decode = decode_mcmc(encode_message, alphabet, matrix_val, char_to_index, iteration=200000)\n",
    "print(f\"Пример 2:\\nОригинальный текст:{text_message}\\nДекодированный текст по mcmc:{message_decode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Бонус: а что если от биграмм перейти к триграммам (тройкам букв) или даже больше? Улучшатся ли результаты? Когда улучшатся, а когда нет? Чтобы ответить на этот вопрос эмпирически, уже может понадобиться погенерировать много тестовых перестановок и последить за метриками, глазами может быть и не видно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем словарь для биграмм\n",
    "def n_gramm(text, n):\n",
    "    \"\"\"Формирование n - грамм.\"\"\"\n",
    "    \n",
    "    ngram_freq = {}\n",
    "    len_text = len(text)\n",
    "    for iterr in range(len_text - 1):\n",
    "        # формируем биграмы по n символам\n",
    "        bigram = text[iterr:iterr + n]\n",
    "        # смотрим их наличие в словаре\n",
    "        if ngram_freq.get(bigram, None):\n",
    "            ngram_freq[bigram] += 1\n",
    "        else:\n",
    "            ngram_freq[bigram] = 1\n",
    "    \n",
    "    # формируем словарь с биграмми в виде ключа и значениями\n",
    "    ngram_freq = dict(sorted(ngram_freq.items(), key=lambda val: val[1], reverse=True))\n",
    "    \n",
    "    return ngram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_ngram(text, dict_text, n):\n",
    "    \"\"\"Функция для декодирования тестовых текстов.\"\"\"\n",
    "\n",
    "    # n-грамы для входного текста\n",
    "    text_freq = n_gramm(text, n)\n",
    "    decoder_text = {key: value for key, value in zip(text_freq, dict_text)}\n",
    "    len_text = len(text)\n",
    "    text_new = []\n",
    "    for iterr in range(0, len_text - 1, n):\n",
    "        text_new.append(text[iterr:iterr + n])\n",
    "    # формируем декодированный текст\n",
    "    text = ''.join(decoder_text[bigram] for bigram in text_new)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество n-грамм: 2\n",
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по биграммам:деасраетчтеро быопач мженитоехбин ув на ити  г ки \n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.4079563182527301\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по биграммам:у внруше э вс нытенаста  ннаош ть  ми нн н ч яниэт\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 0.4115965700285831\n",
      "\n",
      "Количество n-грамм: 3\n",
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по биграммам:о та и этсь  дуложе уобе в сли нувое и алабылскибе\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.41567557028660557\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по биграммам:ую ки а дее у нь обылна овоомуатьговствь итакожее \n",
      "Сходство текста по метрике Левинштейна для 2 примера: 0.4189465087790935\n",
      "\n",
      "Количество n-грамм: 4\n",
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по биграммам:левиенияами не з жизможенимаи ка одн чтого нсти ос\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.42706708268330734\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по биграммам:а пркогоовалный о неого зал  бы ение однень ксанта\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 0.42139648836259697\n",
      "\n",
      "Количество n-грамм: 5\n",
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по биграммам:о он казалходилможноторые лицо и вс его  тепесандр\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.4164554494053422\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по биграммам:ствов старо все бытьость вать ство ностиадьич не в\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 0.4220408163265306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# проходим по различным n-граммам и выводим результаты метрик\n",
    "for iterr in range(2, 6):\n",
    "    # n-грамы для всего текста\n",
    "    ngram_freq = n_gramm(karenina_text, iterr)\n",
    "    decoding_ngram_1 = decode_ngram(encoding_text_1, ngram_freq, n=iterr)\n",
    "    decoding_ngram_2 = decode_ngram(encoding_text_2, ngram_freq, n=iterr)\n",
    "    print(f\"Количество n-грамм: {iterr}\")\n",
    "    print(f\"Пример 1:\\nОригинальный текст:{random_text_1[:50]}\\nДекодированный текст по биграммам:{decoding_ngram_1[:50]}\")\n",
    "    print(f\"Сходство текста по метрике Левинштейна для 1 примера: {ratio(random_text_1, decoding_ngram_1)}\")\n",
    "    print(f\"Пример 2:\\nОригинальный текст:{random_text_2[:50]}\\nДекодированный текст по биграммам:{decoding_ngram_2[:50]}\")\n",
    "    print(f\"Сходство текста по метрике Левинштейна для 2 примера: {ratio(random_text_2, decoding_ngram_2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод:\n",
    "Судя по результатам, лучше всего подходят n-граммы, которые равны 3 и 4, оценка проводилась по метрике Левинштейна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Бонус: какие вы можете придумать применения для этой модели? Пляшущие человечки ведь не так часто встречаются в жизни (хотя встречаются! и это самое потрясающее во всей этой истории, но об этом я расскажу потом)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ответ:\n",
    "\n",
    "Как мне кажется, главное применение может быть для людей, которые только учатся говорить после болезни или маленьких детей, которым сложно говорить, Можно преобразовывать их слова и фразы в текст и дальше применять данный подход. Либо в ситуации, когда сбивается кодировка для восстановления исходного текста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
