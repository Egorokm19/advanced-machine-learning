{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Продвинутое машинное обучение: \n",
    "# Домашнее задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Третье домашнее задание посвящено достаточно простой, но, надеюсь, интересной задаче, в которой потребуется творчески применить методы сэмплирования. Как и раньше, в качестве решения ожидается ссылка на jupyter-ноутбук на вашем github (или публичный, или с доступом для snikolenko); ссылку обязательно нужно прислать в виде сданного домашнего задания на портале Академии. Как всегда, любые комментарии, новые идеи и рассуждения на тему категорически приветствуются. \n",
    "В этом небольшом домашнем задании мы попробуем улучшить метод Шерлока Холмса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пользовался он для этого так называемым частотным методом: смотрел, какие буквы чаще встречаются в зашифрованных текстах, и пытался подставить буквы в соответствии с частотной таблицей: E — самая частая и так далее.\n",
    "В этом задании мы будем разрабатывать более современный и продвинутый вариант такого частотного метода. В качестве корпусов текстов для подсчётов частот можете взять что угодно, но для удобства вот вам “Война и мир” по-русски и по-английски:\n",
    "https://www.dropbox.com/s/k23enjvr3fb40o5/corpora.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Реализуйте базовый частотный метод по Шерлоку Холмсу:\n",
    "- подсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "- возьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе вряд ли сработает), зашифруйте их посредством случайной перестановки символов;\n",
    "- расшифруйте их таким частотным методом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import random\n",
    "import collections\n",
    "import warnings\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from Levenshtein import ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение zip архива\n",
    "corpora_zip = zipfile.ZipFile('corpora.zip')\n",
    "\n",
    "# загрузка файла AnnaKarenina\n",
    "with corpora_zip.open('AnnaKarenina.txt', 'r') as readFile:\n",
    "    karenina_text = readFile.read().decode('utf8')\n",
    "# загрузка файла AnnaKarenina\n",
    "with corpora_zip.open('WarAndPeace.txt', 'r') as readFile:\n",
    "    war_peace_text = readFile.read().decode('utf8')\n",
    "# загрузка файла AnnaKarenina\n",
    "with corpora_zip.open('WarAndPeaceEng.txt', 'r') as readFile:\n",
    "    war_peace_eng_text = readFile.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, language):\n",
    "    \"\"\"Обработка текста.\"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    if language == 'russian':\n",
    "        text = re.sub(r'[^а-яА-ЯыъьёЁ]', \" \", text)\n",
    "    else:\n",
    "        text = re.sub(r'[^a-zA-Z]', \" \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обрабатываем все файлы\n",
    "karenina_text = preprocessing(karenina_text, language='russian')\n",
    "war_peace_text = preprocessing(war_peace_text, language='russian')\n",
    "war_peace_eng_text = preprocessing(war_peace_eng_text, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подсчитаем частоты букв по корпусам\n",
    "val_freq = collections.Counter(karenina_text)\n",
    "karenina_freq = {key: val for key, val in val_freq.items() if key in set(karenina_text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальный текст: левин думал о евангельском изречении не потому что\n",
      "Закодированный текст: иьпощеаэъриеёеьпрщфьимбзёъеотуьдьщооещьесёяёъэедяё\n"
     ]
    }
   ],
   "source": [
    "# берем рандомный текст и делаем перестановку символов\n",
    "random_text_1 = \"\"\"Левин думал о евангельском изречении не потому, чтоб он считал себя премудрым. Он не считал себя премудрым, но не мог не знать, что он был умнее жены и Агафьи Михайловны, и не мог не знать того, что, когда он думал о смерти, он думал всеми силами души. Он знал тоже, что многие мужские большие умы, мысли которых об этом он читал, думали об этом и не знали одной сотой того, что знала об этом его жена и Агафья Михайловна. Как ни различны были эти две женщины, Агафья Михайловна и Катя, как ее называл брат Николай и как теперь Левину было особенно приятно называть ее, они в этом были совершенно похожи. Обе несомненно знали, что такое была жизнь и что такое была смерть, и хотя никак не могли ответить и не поняли бы даже тех вопросов, которые представлялись Левину, обе не сомневались в значении этого явления и совершенно одинаково, не только между собой, но разделяя этот взгляд с миллионами людей, смотрели на это. Доказательство того, что они знали твердо, что такое была смерть, состояло в том, что они, ни секунды не сомневаясь, знали, как надо действовать с умирающими, и не боялись их. Левин же и другие, хотя и многое могли сказать о смерти, очевидно не знали, потому что боялись смерти и решительно не знали, что надо делать, когда люди умирают. Если бы Левин был теперь один с братом Николаем, он бы с ужасом смотрел на него и еще с бóльшим ужасом ждал, и больше ничего бы не умел сделать.\n",
    "Мало того, он не знал, что говорить, как смотреть, как ходить. Говорить о постороннем ему казалось оскорбительным, нельзя; говорить о смерти, о мрачном – тоже нельзя. Молчать – тоже нельзя. «Смотреть – он подумает, что я изучаю его, боюсь; не смотреть – он подумает, что я о другом думаю. Ходить на цыпочках – он будет недоволен; на всю ногу – совестно». Кити же, очевидно, не думала и не имела времени думать о себе; она думала о нем, потому что знала что-то, и все выходило хорошо. Она и про себя рассказывала и про свою свадьбу, и улыбалась, и жалела, и ласкала его, и говорила о случаях выздоровления, и все выходило хорошо; стало быть, она знала. Доказательством того, что деятельность ее и Агафьи Михайловны была не инстинктивная, животная, неразумная, было то, что, кроме физического ухода, облегчения страданий, и Агафья Михайловна и Кити требовали для умирающего еще чего-то такого, более важного, чем физический уход, и чего-то такого, что не имело ничего общего с условиями физическими. Агафья Михайловна, говоря об умершем старике, сказала: «Что ж, слава Богу, причастили, соборовали, дай Бог каждому так умереть». Катя точно так же, кроме всех забот о белье, пролежнях, питье, в первый же день успела уговорить больного в необходимости причаститься и собороваться.\"\"\"\n",
    "random_text_2 = \"\"\"Вернувшись от больного на ночь в свои два нумера, Левин сидел, опустив голову, не зная, что делать. Не говоря уже о том, чтоб ужинать, устраиваться на ночлег, обдумывать, что они будут делать, он даже и говорить с женою не мог: ему совестно было. Кити же, напротив, была деятельнее обыкновенного. Она даже была оживленнее обыкновенного. Она велела принести ужинать, сама разобрала вещи, сама помогла стлать постели и не забыла обсыпать их персидским порошком. В ней было возбуждение и быстрота соображения, которые появляются у мужчин пред сражением, борьбой, в опасные и решительные минуты жизни, те минуты, когда раз навсегда мужчина показывает свою цену и то, что все прошедшее его было не даром, а приготовлением к этим минутам.\n",
    "Все дело спорилось у нее, и еще не было двенадцати, как все вещи были разобраны чисто, аккуратно, как-то так особенно, что нумер стал похож на дом, на ее комнаты: постели постланы, щетки, гребни, зеркальца выложены, салфеточки постланы.\n",
    "Левин находил, что непростительно есть, спать, говорить даже теперь, и чувствовал, что каждое движение его было неприлично. Она же разбирала щеточки, но делала все это так, что ничего в этом оскорбительного не было.\n",
    "Есть, однако, они ничего не могли, и долго не могли заснуть, и даже долго не ложились спать.\"\"\"\n",
    "\n",
    "# предобработаем их\n",
    "random_text_1 = preprocessing(random_text_1, language='russian')\n",
    "random_text_2 = preprocessing(random_text_2, language='russian')\n",
    "\n",
    "# перемешиваем и получаем рандомные значений\n",
    "shuffle_text = sorted(list(set(karenina_text)))\n",
    "# перемешиваем\n",
    "random.shuffle(shuffle_text)\n",
    "encoding_text = {key: value for key, value in zip(sorted(list(set(karenina_text))), shuffle_text)}\n",
    "\n",
    "# кодируем текст\n",
    "encoding_text_1 = ''.join(encoding_text[txt] for txt in random_text_1)\n",
    "encoding_text_2 = ''.join(encoding_text[txt] for txt in random_text_2)\n",
    "print(f\"Оригинальный текст: {random_text_1[:50]}\\nЗакодированный текст: {encoding_text_1[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# декодируем последовательность\n",
    "def decode_text(text, dict_text):\n",
    "    \"\"\"Декодирование последовательности.\"\"\"\n",
    "    \n",
    "    val_freq = collections.Counter(text)\n",
    "    text_freq = {key: val for key, val in val_freq.items() if key in set(karenina_text)}\n",
    "    sort_text_freq = dict(val_freq.most_common())\n",
    "    letter_pairs = {k: v for k, v in zip(sort_text_freq, dict_text)}\n",
    "    decoded_text = ''.join(letter_pairs[let] for let in text)\n",
    "    \n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст:серни мьлас о ераияесдвуол нбкеыеинн ие жотоль ыто\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.5097503900156006\n",
      "\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст:вераявйилм от досмаого ан аочм в лвои квн аяперн с\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 0.5967346938775511\n"
     ]
    }
   ],
   "source": [
    "# формируем словарь \n",
    "dict_text = dict(val_freq.most_common())\n",
    "decoding_text_1 = decode_text(encoding_text_1, dict_text)\n",
    "decoding_text_2 = decode_text(encoding_text_2, dict_text)\n",
    "print(f\"Пример 1:\\nОригинальный текст:{random_text_1[:50]}\\nДекодированный текст:{decoding_text_1[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 1 примера: {ratio(random_text_1, decoding_text_1)}\")\n",
    "print(f\"\\nПример 2:\\nОригинальный текст:{random_text_2[:50]}\\nДекодированный текст:{decoding_text_2[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 2 примера: {ratio(random_text_2, decoding_text_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод: фразы распознаются, но довольно трудно понять смысл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:\n",
    "- подсчитайте частоты биграмм (т.е. пар последовательных букв) по корпусам;\n",
    "- проведите тестирование аналогично п.1, но при помощи биграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем словарь для биграмм\n",
    "def n_gramm(text, n):\n",
    "    \"\"\"Формирование n - грамм.\"\"\"\n",
    "    \n",
    "    ngram_freq = {}\n",
    "    len_text = len(text)\n",
    "    for iterr in range(len_text - 1):\n",
    "        # формируем биграмы по n символам\n",
    "        bigram = text[iterr:iterr + n]\n",
    "        # смотрим их наличие в словаре\n",
    "        if ngram_freq.get(bigram, None):\n",
    "            ngram_freq[bigram] += 1\n",
    "        else:\n",
    "            ngram_freq[bigram] = 1\n",
    "    \n",
    "    # формируем словарь с биграмми в виде ключа и значениями\n",
    "    ngram_freq = dict(sorted(ngram_freq.items(), key=lambda val: val[1], reverse=True))\n",
    "    \n",
    "    return ngram_freq\n",
    "\n",
    "# биграмы для всего текста\n",
    "bigram_freq = n_gramm(karenina_text, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_ngram(text, dict_text, n):\n",
    "    \"\"\"Функция для декодирования тестовых текстов.\"\"\"\n",
    "\n",
    "    # n-грамы для входного текста\n",
    "    text_freq = n_gramm(text, n)\n",
    "    decoder_text = {key: value for key, value in zip(text_freq, dict_text)}\n",
    "    len_text = len(text)\n",
    "    text_new = []\n",
    "    for iterr in range(0, len_text - 1, n):\n",
    "        text_new.append(text[iterr:iterr + n])\n",
    "    # формируем декодированный текст\n",
    "    text = ''.join(decoder_text[bigram] for bigram in text_new)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по биграммам:деасраетчтеро быопач мженитоехбин ув на ити  г ки \n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.4079563182527301\n",
      "\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по биграммам:у внруше э вс нытенаста  ннаош ть  ми нн н ч яниэт\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 0.4115965700285831\n"
     ]
    }
   ],
   "source": [
    "# декодируем текст\n",
    "decoding_bigram_1 = decode_ngram(encoding_text_1, bigram_freq, 2)\n",
    "decoding_bigram_2 = decode_ngram(encoding_text_2, bigram_freq, 2)\n",
    "print(f\"Пример 1:\\nОригинальный текст:{random_text_1[:50]}\\nДекодированный текст по биграммам:{decoding_bigram_1[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 1 примера: {ratio(random_text_1, decoding_bigram_1)}\")\n",
    "print(f\"\\nПример 2:\\nОригинальный текст:{random_text_2[:50]}\\nДекодированный текст по биграммам:{decoding_bigram_2[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 2 примера: {ratio(random_text_2, decoding_bigram_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод: результаты стали хуже, кодировалось по одному методу, а декодируется по другому"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "- предложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "- реализуйте и протестируйте его, убедитесь, что результаты улучшились."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC-сэмплирование\n",
    "# все символы текста\n",
    "alphabet = ''.join(sorted(list(set(karenina_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(text, freq, n_gramm_val):\n",
    "    \"\"\"Определяем функцию правдоподобия.\"\"\"\n",
    "    \n",
    "    bigram_val = n_gramm(''.join(text), n_gramm_val)\n",
    "    calculation_log_likelihood = np.sum([val * np.log(freq.get(key, 1 / len(alphabet) ** 2)) for key, val in bigram_val.items()])\n",
    "    \n",
    "    return calculation_log_likelihood\n",
    "\n",
    "def get_permutation(alphabet, text):\n",
    "    \"\"\"Перестановка значений.\"\"\"\n",
    "    \n",
    "    permut = np.random.choice(list(alphabet), 2, replace=False)\n",
    "    for iterr in range(len(text)):\n",
    "        if text[iterr] == permut[0]:\n",
    "            text[iterr] = permut[1]\n",
    "        elif text[iterr] == permut[1]:\n",
    "            text[iterr] = permut[0]\n",
    "            \n",
    "    return text\n",
    "\n",
    "def accept(value_likelihood, new_value_likelihood):\n",
    "    \"\"\"Проверка значения максимального правдоподобия.\"\"\"\n",
    "    \n",
    "    if new_value_likelihood > value_likelihood:\n",
    "        return True\n",
    "    else:\n",
    "        return np.random.rand() < np.exp(new_value_likelihood - value_likelihood)\n",
    "\n",
    "def decode(text, alphabet, freq, n_gramm_val, n_iters):\n",
    "    \"\"\"Функция декодирования по MCMC.\"\"\"\n",
    "    \n",
    "    decoder_text = copy(text)\n",
    "    value_likelihood = best_value = log_likelihood(text, freq, n_gramm_val)\n",
    "    for iteration in tqdm(range(n_iters)):\n",
    "        new_text = get_permutation(alphabet, copy(text))\n",
    "        new_value_likelihood = log_likelihood(new_text, freq, n_gramm_val)\n",
    "        if accept(value_likelihood, new_value_likelihood):\n",
    "            text = new_text\n",
    "            value_likelihood = new_value_likelihood\n",
    "            if value_likelihood > best_value:\n",
    "                best_value = value_likelihood\n",
    "                decoder_text = copy(text)\n",
    "                \n",
    "    out_text = ''.join(decoder_text)\n",
    "    \n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:25<00:00, 117.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по mcmc:левин думал о евангельском изречении не потому что\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# декодируем последовательность\n",
    "decoded_mcc_text_1 = decode(list(encoding_text_1), alphabet, bigram_freq, n_gramm_val=2, n_iters=10000)\n",
    "print(f\"Пример 1:\\nОригинальный текст:{random_text_1[:50]}\\nДекодированный текст по mcmc:{decoded_mcc_text_1[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 1 примера: {ratio(random_text_1, decoded_mcc_text_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [01:10<00:00, 213.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 1:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по mcmc:вернувшись от больного на ночь в свои два нумера л\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# декодируем последовательность\n",
    "decoded_mcc_text_2 = decode(list(encoding_text_2), alphabet, bigram_freq, n_gramm_val=2, n_iters=15000)\n",
    "print(f\"Пример 1:\\nОригинальный текст:{random_text_2[:50]}\\nДекодированный текст по mcmc:{decoded_mcc_text_2[:50]}\")\n",
    "print(f\"Сходство текста по метрике Левинштейна для 1 примера: {ratio(random_text_2, decoded_mcc_text_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Расшифруйте сообщение:\n",
    "←⇠⇒↟↹↷⇊↹↷↟↤↟↨←↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↟⇒↟↹⇷⇛⇞↨↟↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↨←⇌⇠↨↹⇙↹⇸↨⇛↙⇛↹⇠⇛⇛↲⇆←↝↟↞↹⇌⇛↨⇛⇯⇊↾↹⇒←↙⇌⇛↹⇷⇯⇛⇞↟↨⇴↨⇈↹⇠⇌⇛⇯←←↹↷⇠←↙⇛↹↷⇊↹↷⇠←↹⇠↤←⇒⇴⇒↟↹⇷⇯⇴↷↟⇒⇈↝⇛↹↟↹⇷⇛⇒⇙⇞↟↨←↹↳⇴⇌⇠↟↳⇴⇒⇈↝⇊↾↹↲⇴⇒⇒↹⇰⇴↹⇷⇛⇠⇒←↤↝←←↹⇞←↨↷←⇯↨⇛←↹⇰⇴↤⇴↝↟←↹⇌⇙⇯⇠⇴↹↘⇛↨↞↹⇌⇛↝←⇞↝⇛↹↞↹↝↟⇞←↙⇛↹↝←↹⇛↲←⇆⇴⇏\n",
    "Или это (они одинаковые, второй вариант просто на случай проблем с юникодом):\n",
    "დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭჾႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭჾႣჵისႼჰႨჲდႩჳჲႨჇႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵჇႧჂჲდႨჾႣႩჳჂჾႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხსდდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩჇႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [03:16<00:00, 1017.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 2:\n",
      "Оригинальный текст:დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭჾႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭჾႣჵისႼჰႨჲდႩჳჲႨჇႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵჇႧჂჲდႨჾႣႩჳჂჾႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხსდდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩჇႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ\n",
      "Декодированный текст по mcmc:если вы вимите нордальный или почти нордальный текст у этого сообщения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный балл за послемнее четвертое замание курса хотя конечно я ничего не обещаж\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# текстовое сообщение\n",
    "text_message = \"დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭჾႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭჾႣჵისႼჰႨჲდႩჳჲႨჇႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵჇႧჂჲდႨჾႣႩჳჂჾႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხსდდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩჇႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ\"\n",
    "# преобразуем последовательность\n",
    "list_message_symbol = list(set(text_message))\n",
    "decoder_alphabet = {key: value for key, value in zip(list_message_symbol, alphabet)}\n",
    "encode_message = ''.join(decoder_alphabet[char] for char in text_message)\n",
    "# декодируем сообщение\n",
    "message_decode = decode(list(encode_message), alphabet, bigram_freq, n_gramm_val=2, n_iters=200000)\n",
    "print(f\"Пример 2:\\nОригинальный текст:{text_message}\\nДекодированный текст по mcmc:{message_decode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Это получается из-за того, что в нашей закодированной последовательности 28 символов, а в исходном словаре 34 символа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Бонус: а что если от биграмм перейти к триграммам (тройкам букв) или даже больше? Улучшатся ли результаты? Когда улучшатся, а когда нет? Чтобы ответить на этот вопрос эмпирически, уже может понадобиться погенерировать много тестовых перестановок и последить за метриками, глазами может быть и не видно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [01:43<00:00, 145.14it/s]\n",
      "100%|██████████| 15000/15000 [00:54<00:00, 276.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество n-грамм: 2\n",
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по биграммам:нрксдеблоине еркидярнытм оесжарзрдсседрех в олезв \n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.36076443057722307\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по биграммам:вернувшись от больного на ночь в свои два нумера л\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [02:14<00:00, 111.93it/s]\n",
      "100%|██████████| 15000/15000 [01:15<00:00, 198.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество n-грамм: 3\n",
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по биграммам:ивяле споми н вямеывиткбно лчувзвелл ев гнаноп зан\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.3841653666146646\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по биграммам:вернувшись от больного на ночь в свои два нумера л\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [02:39<00:00, 94.12it/s] \n",
      "100%|██████████| 15000/15000 [01:25<00:00, 176.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество n-грамм: 4\n",
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по биграммам:инско хмети л нстоыниявэле кпрнднокк он глалем дал\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.39430577223088925\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по биграммам:илыенищвзм та утометст ер етдм и зитв пир еньлыр о\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 0.39183673469387753\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [02:51<00:00, 87.49it/s]\n",
      "100%|██████████| 15000/15000 [01:32<00:00, 162.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество n-грамм: 5\n",
      "Пример 1:\n",
      "Оригинальный текст:левин думал о евангельском изречении не потому что\n",
      "Декодированный текст по биграммам:арылеиуго аитиры еврапшйтоилкярнреллиериътстогинст\n",
      "Сходство текста по метрике Левинштейна для 1 примера: 0.34321372854914195\n",
      "Пример 2:\n",
      "Оригинальный текст:вернувшись от больного на ночь в свои два нумера л\n",
      "Декодированный текст по биграммам:хтанкхпрлбежоеджибнжщжен енжмбехелхжрейх енкута еи\n",
      "Сходство текста по метрике Левинштейна для 2 примера: 0.3526530612244898\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# проходим по различным n-граммам и выводим результаты метрик\n",
    "for iterr in range(2, 6):\n",
    "    # n-грамы для всего текста\n",
    "    ngram_freq = n_gramm(karenina_text, iterr)\n",
    "    # декодируем последовательность\n",
    "    decoding_ngram_1 = decode(list(encoding_text_1), alphabet, ngram_freq, n_gramm_val=iterr, n_iters=15000)\n",
    "    decoding_ngram_2 = decode(list(encoding_text_2), alphabet, ngram_freq, n_gramm_val=iterr, n_iters=15000)\n",
    "    print(f\"Количество n-грамм: {iterr}\")\n",
    "    print(f\"Пример 1:\\nОригинальный текст:{random_text_1[:50]}\\nДекодированный текст по биграммам:{decoding_ngram_1[:50]}\")\n",
    "    print(f\"Сходство текста по метрике Левинштейна для 1 примера: {ratio(random_text_1, decoding_ngram_1)}\")\n",
    "    print(f\"Пример 2:\\nОригинальный текст:{random_text_2[:50]}\\nДекодированный текст по биграммам:{decoding_ngram_2[:50]}\")\n",
    "    print(f\"Сходство текста по метрике Левинштейна для 2 примера: {ratio(random_text_2, decoding_ngram_2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод:\n",
    "Судя по результатам, лучше всего подходят n-граммы, которые равны 3, оценка проводилась по метрике Левинштейна. Ещее необходимо учесть, что эксперимент проводился на 15000 итераций, возможно при добавлении итераций, метрики улучшаться. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Бонус: какие вы можете придумать применения для этой модели? Пляшущие человечки ведь не так часто встречаются в жизни (хотя встречаются! и это самое потрясающее во всей этой истории, но об этом я расскажу потом)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ответ:\n",
    "\n",
    "Как мне кажется, главное применение может быть для людей, которые только учатся говорить после болезни или маленьких детей, которым сложно говорить, Можно преобразовывать их слова и фразы в текст и дальше применять данный подход. Либо в ситуации, когда сбивается кодировка для восстановления исходного текста. Также может применяться в последовательностях ДНК."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
